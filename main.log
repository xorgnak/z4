[1711051692] Log start
[1711051692] Cmd: /usr/local/bin/llama -m tinyllama-1.1b-chat-v1.0.Q4_0.gguf -c 2048 -b 128 -p "Nutrition Awareness Month
Colorectal Cancer Awareness Month
HIV/AIDS Awareness Month
Thursday - Ladies Night
Friday - Guys Night
Going means persuading him was easy going.
Tonight means the present or immediately coming night.
What's going on tonight?

"
[1711051697] main: seed  = 1711051697
[1711051697] main: llama backend init
[1711051698] main: load the model and apply lora adapter, if any
[1711051698] llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from tinyllama-1.1b-chat-v1.0.Q4_0.gguf (version GGUF V3 (latest))
[1711051698] llama_model_loader: - tensor    0:                    output.weight q6_K     [  2048, 32000,     1,     1 ]
[1711051698] llama_model_loader: - tensor    1:                token_embd.weight q4_0     [  2048, 32000,     1,     1 ]
[1711051698] llama_model_loader: - tensor    2:           blk.0.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor    3:            blk.0.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor    4:            blk.0.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor    5:              blk.0.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor    6:            blk.0.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor    7:              blk.0.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor    8:         blk.0.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor    9:              blk.0.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   10:              blk.0.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   11:           blk.1.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   12:            blk.1.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   13:            blk.1.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   14:              blk.1.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   15:            blk.1.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   16:              blk.1.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   17:         blk.1.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   18:              blk.1.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   19:              blk.1.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   20:          blk.10.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   21:           blk.10.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   22:           blk.10.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   23:             blk.10.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   24:           blk.10.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   25:             blk.10.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   26:        blk.10.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   27:             blk.10.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   28:             blk.10.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   29:          blk.11.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   30:           blk.11.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   31:           blk.11.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   32:             blk.11.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   33:           blk.11.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   34:             blk.11.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   35:        blk.11.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   36:             blk.11.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   37:             blk.11.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   38:          blk.12.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   39:           blk.12.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   40:           blk.12.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   41:             blk.12.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   42:           blk.12.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   43:             blk.12.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   44:        blk.12.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   45:             blk.12.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   46:             blk.12.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   47:          blk.13.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   48:           blk.13.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   49:           blk.13.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   50:             blk.13.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   51:           blk.13.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   52:             blk.13.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   53:        blk.13.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   54:             blk.13.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   55:             blk.13.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   56:          blk.14.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   57:           blk.14.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   58:           blk.14.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   59:             blk.14.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   60:           blk.14.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   61:             blk.14.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   62:        blk.14.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   63:             blk.14.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   64:             blk.14.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   65:          blk.15.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   66:           blk.15.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   67:           blk.15.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   68:             blk.15.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   69:           blk.15.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   70:             blk.15.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   71:        blk.15.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   72:             blk.15.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   73:             blk.15.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   74:          blk.16.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   75:           blk.16.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   76:           blk.16.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   77:             blk.16.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   78:           blk.16.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   79:             blk.16.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   80:        blk.16.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   81:             blk.16.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   82:             blk.16.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   83:          blk.17.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   84:           blk.17.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   85:           blk.17.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   86:             blk.17.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   87:           blk.17.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   88:             blk.17.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   89:        blk.17.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   90:             blk.17.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   91:             blk.17.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   92:          blk.18.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   93:           blk.18.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   94:           blk.18.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   95:             blk.18.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor   96:           blk.18.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor   97:             blk.18.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor   98:        blk.18.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor   99:             blk.18.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  100:             blk.18.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  101:          blk.19.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  102:           blk.19.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  103:           blk.19.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  104:             blk.19.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  105:           blk.19.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  106:             blk.19.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  107:        blk.19.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  108:             blk.19.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  109:             blk.19.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  110:           blk.2.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  111:            blk.2.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  112:            blk.2.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  113:              blk.2.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  114:            blk.2.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  115:              blk.2.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  116:         blk.2.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  117:              blk.2.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  118:              blk.2.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  119:          blk.20.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  120:           blk.20.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  121:           blk.20.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  122:             blk.20.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  123:           blk.20.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  124:             blk.20.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  125:        blk.20.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  126:             blk.20.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  127:             blk.20.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  128:          blk.21.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  129:           blk.21.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  130:           blk.21.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  131:             blk.21.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  132:           blk.21.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  133:             blk.21.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  134:        blk.21.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  135:             blk.21.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  136:             blk.21.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  137:           blk.3.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  138:            blk.3.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  139:            blk.3.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  140:              blk.3.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  141:            blk.3.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  142:              blk.3.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  143:         blk.3.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  144:              blk.3.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  145:              blk.3.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  146:           blk.4.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  147:            blk.4.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  148:            blk.4.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  149:              blk.4.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  150:            blk.4.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  151:              blk.4.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  152:         blk.4.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  153:              blk.4.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  154:              blk.4.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  155:           blk.5.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  156:            blk.5.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  157:            blk.5.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  158:              blk.5.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  159:            blk.5.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  160:              blk.5.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  161:         blk.5.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  162:              blk.5.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  163:              blk.5.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  164:           blk.6.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  165:            blk.6.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  166:            blk.6.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  167:              blk.6.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  168:            blk.6.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  169:              blk.6.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  170:         blk.6.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  171:              blk.6.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  172:              blk.6.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  173:           blk.7.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  174:            blk.7.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  175:            blk.7.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  176:              blk.7.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  177:            blk.7.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  178:              blk.7.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  179:         blk.7.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  180:              blk.7.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  181:              blk.7.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  182:           blk.8.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  183:            blk.8.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  184:            blk.8.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  185:              blk.8.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  186:            blk.8.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  187:              blk.8.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  188:         blk.8.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  189:              blk.8.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  190:              blk.8.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  191:           blk.9.attn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  192:            blk.9.ffn_down.weight q4_0     [  5632,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  193:            blk.9.ffn_gate.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  194:              blk.9.ffn_up.weight q4_0     [  2048,  5632,     1,     1 ]
[1711051698] llama_model_loader: - tensor  195:            blk.9.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: - tensor  196:              blk.9.attn_k.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  197:         blk.9.attn_output.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  198:              blk.9.attn_q.weight q4_0     [  2048,  2048,     1,     1 ]
[1711051698] llama_model_loader: - tensor  199:              blk.9.attn_v.weight q4_0     [  2048,   256,     1,     1 ]
[1711051698] llama_model_loader: - tensor  200:               output_norm.weight f32      [  2048,     1,     1,     1 ]
[1711051698] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1711051698] llama_model_loader: - kv   0:                       general.architecture str              = llama
[1711051698] llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0
[1711051698] llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
[1711051698] llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048
[1711051698] llama_model_loader: - kv   4:                          llama.block_count u32              = 22
[1711051698] llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632
[1711051698] llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64
[1711051698] llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
[1711051698] llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4
[1711051698] llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
[1711051698] llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000
[1711051698] llama_model_loader: - kv  11:                          general.file_type u32              = 2
[1711051698] llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
[1711051698] llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
[1711051698] llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
[1711051698] llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
[1711051698] llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = ["▁ t", "e r", "i n", "▁ a", "e n...
[1711051698] llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
[1711051698] llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
[1711051698] llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
[1711051698] llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2
[1711051698] llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\n{% if m...
[1711051698] llama_model_loader: - kv  22:               general.quantization_version u32              = 2
[1711051698] llama_model_loader: - type  f32:   45 tensors
[1711051698] llama_model_loader: - type q4_0:  155 tensors
[1711051698] llama_model_loader: - type q6_K:    1 tensors
[1711051698] llm_load_vocab: special tokens definition check successful ( 259/32000 ).
[1711051698] llm_load_print_meta: format           = GGUF V3 (latest)
[1711051698] llm_load_print_meta: arch             = llama
[1711051698] llm_load_print_meta: vocab type       = SPM
[1711051698] llm_load_print_meta: n_vocab          = 32000
[1711051698] llm_load_print_meta: n_merges         = 0
[1711051698] llm_load_print_meta: n_ctx_train      = 2048
[1711051698] llm_load_print_meta: n_embd           = 2048
[1711051698] llm_load_print_meta: n_head           = 32
[1711051698] llm_load_print_meta: n_head_kv        = 4
[1711051698] llm_load_print_meta: n_layer          = 22
[1711051698] llm_load_print_meta: n_rot            = 64
[1711051698] llm_load_print_meta: n_gqa            = 8
[1711051698] llm_load_print_meta: f_norm_eps       = 0.0e+00
[1711051698] llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
[1711051698] llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[1711051698] llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[1711051698] llm_load_print_meta: n_ff             = 5632
[1711051698] llm_load_print_meta: n_expert         = 0
[1711051698] llm_load_print_meta: n_expert_used    = 0
[1711051698] llm_load_print_meta: rope scaling     = linear
[1711051698] llm_load_print_meta: freq_base_train  = 10000.0
[1711051698] llm_load_print_meta: freq_scale_train = 1
[1711051698] llm_load_print_meta: n_yarn_orig_ctx  = 2048
[1711051698] llm_load_print_meta: rope_finetuned   = unknown
[1711051698] llm_load_print_meta: model type       = ?B
[1711051698] llm_load_print_meta: model ftype      = mostly Q4_0
[1711051698] llm_load_print_meta: model params     = 1.10 B
[1711051698] llm_load_print_meta: model size       = 606.53 MiB (4.63 BPW) 
[1711051698] llm_load_print_meta: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0
[1711051698] llm_load_print_meta: BOS token        = 1 '<s>'
[1711051698] llm_load_print_meta: EOS token        = 2 '</s>'
[1711051698] llm_load_print_meta: UNK token        = 0 '<unk>'
[1711051698] llm_load_print_meta: PAD token        = 2 '</s>'
[1711051698] llm_load_print_meta: LF token         = 13 '<0x0A>'
[1711051698] llm_load_tensors: ggml ctx size =    0.08 MiB
[1711051698] llm_load_tensors: mem required  =  606.61 MiB
[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] .[1711051698] 
[1711051698] llama_new_context_with_model: n_ctx      = 2048
[1711051698] llama_new_context_with_model: freq_base  = 10000.0
[1711051698] llama_new_context_with_model: freq_scale = 1
[1711051698] llama_new_context_with_model: KV self size  =   44.00 MiB, K (f16):   22.00 MiB, V (f16):   22.00 MiB
[1711051698] llama_build_graph: non-view tensors processed: 466/466
[1711051698] llama_new_context_with_model: compute buffer total size = 39.31 MiB
[1711051698] llama_new_context_with_model: VRAM scratch buffer: 36.00 MiB
[1711051698] llama_new_context_with_model: total VRAM used: 36.00 MiB (model: 0.00 MiB, context: 36.00 MiB)
[1711051698] warming up the model with an empty run
[1711051730] n_ctx: 2048
[1711051730] 
[1711051730] system_info: n_threads = 4 / 4 | AVX = 1 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
[1711051730] add_bos: 1
[1711051730] tokenize the prompt
[1711051731] prompt: "Nutrition Awareness Month
Colorectal Cancer Awareness Month
HIV/AIDS Awareness Month
Thursday - Ladies Night
Friday - Guys Night
Going means persuading him was easy going.
Tonight means the present or immediately coming night.
What's going on tonight?

"
[1711051731] tokens: [ '':1, ' Nu':12487, 'tri':3626, 'tion':12757, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Color':3306, 'ect':522, 'al':284, ' Can':1815, 'cer':2265, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'HI':17628, 'V':29963, '/':29914, 'AI':23869, 'DS':8452, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13 ]
[1711051731] recalculate the cached logits (check): embd_inp.empty() false, n_matching_session_tokens 0, embd_inp.size() 82, session_tokens.size() 0, embd_inp.size() 82
[1711051731] inp_pfx: [ '':1, ' ':29871, '':13, '':13, '##':2277, '#':29937, ' Inst':2799, 'ruction':4080, ':':29901, '':13, '':13 ]
[1711051731] inp_sfx: [ ' ':29871, '':13, '':13, '##':2277, '#':29937, ' Response':13291, ':':29901, '':13, '':13 ]
[1711051731] cml_pfx: [ '':1, ' ':29871, '':13, '<':29966, '|':29989, 'im':326, '_':29918, 'start':2962, '|':29989, '>':29958, 'user':1792, '':13 ]
[1711051731] cml_sfx: [ ' <':529, '|':29989, 'im':326, '_':29918, 'end':355, '|':29989, '>':29958, '':13, '<':29966, '|':29989, 'im':326, '_':29918, 'start':2962, '|':29989, '>':29958, 'ass':465, 'istant':22137, '':13 ]
[1711051731] sampling: 
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
[1711051731] sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp 
[1711051731] generate: n_ctx = 2048, n_batch = 128, n_predict = -1, n_keep = 0
[1711051731] 

[1711051731] embd_inp.size(): 82, n_consumed: 0
[1711051731] eval: [ '':1, ' Nu':12487, 'tri':3626, 'tion':12757, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Color':3306, 'ect':522, 'al':284, ' Can':1815, 'cer':2265, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'HI':17628, 'V':29963, '/':29914, 'AI':23869, 'DS':8452, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13 ]
[1711051791] n_past = 82
[1711051791] sampled token: 29940: 'N'
[1711051791] last: [ 'HI':17628, 'V':29963, '/':29914, 'AI':23869, 'DS':8452, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940 ]
[1711051791] n_remain: -2
[1711051791] eval: [ 'N':29940 ]
[1711051849] n_past = 83
[1711051850] sampled token:   523: 'ight'
[1711051850] last: [ 'V':29963, '/':29914, 'AI':23869, 'DS':8452, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523 ]
[1711051850] n_remain: -3
[1711051850] eval: [ 'ight':523 ]
[1711051893] n_past = 84
[1711051894] sampled token:  8152: ' ow'
[1711051894] last: [ '/':29914, 'AI':23869, 'DS':8452, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523, ' ow':8152 ]
[1711051894] n_remain: -4
[1711051894] eval: [ ' ow':8152 ]
[1711051937] n_past = 85
[1711051937] sampled token: 29880: 'l'
[1711051937] last: [ 'AI':23869, 'DS':8452, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523, ' ow':8152, 'l':29880 ]
[1711051937] n_remain: -5
[1711051937] eval: [ 'l':29880 ]
[1711051980] n_past = 86
[1711051980] sampled token: 29892: ','
[1711051980] last: [ 'DS':8452, ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523, ' ow':8152, 'l':29880, ',':29892 ]
[1711051980] n_remain: -6
[1711051980] eval: [ ',':29892 ]
[1711052023] n_past = 87
[1711052023] sampled token:   306: ' I'
[1711052023] last: [ ' Aw':22886, 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523, ' ow':8152, 'l':29880, ',':29892, ' I':306 ]
[1711052023] n_remain: -7
[1711052023] eval: [ ' I':306 ]
[1711052067] n_past = 88
[1711052067] sampled token:  5360: ' love'
[1711052067] last: [ 'aren':8326, 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523, ' ow':8152, 'l':29880, ',':29892, ' I':306, ' love':5360 ]
[1711052067] n_remain: -8
[1711052067] eval: [ ' love':5360 ]
[1711052138] n_past = 89
[1711052139] sampled token:   366: ' you'
[1711052139] last: [ 'ess':404, ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523, ' ow':8152, 'l':29880, ',':29892, ' I':306, ' love':5360, ' you':366 ]
[1711052139] n_remain: -9
[1711052139] eval: [ ' you':366 ]
[1711052200] n_past = 90
[1711052201] sampled token: 29892: ','
[1711052201] last: [ ' Month':23471, '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523, ' ow':8152, 'l':29880, ',':29892, ' I':306, ' love':5360, ' you':366, ',':29892 ]
[1711052201] n_remain: -10
[1711052201] eval: [ ',':29892 ]
[1711052282] n_past = 91
[1711052282] sampled token:  4646: ' night'
[1711052282] last: [ '':13, 'Th':1349, 'urs':1295, 'day':3250, ' -':448, ' Lad':19955, 'ies':583, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Fri':27034, 'day':3250, ' -':448, ' Guy':15880, 's':29879, ' Ni':15981, 'gh':12443, 't':29873, '':13, 'Go':8120, 'ing':292, ' means':2794, ' persu':20408, 'ading':9382, ' him':1075, ' was':471, ' easy':4780, ' going':2675, '.':29889, '':13, 'To':1762, 'night':11147, ' means':2794, ' the':278, ' present':2198, ' or':470, ' immediately':7389, ' coming':6421, ' ni':6836, 'gh':12443, 't':29873, '.':29889, '':13, 'What':5618, ''':29915, 's':29879, ' going':2675, ' on':373, ' ton':15243, 'ight':523, '?':29973, '':13, '':13, 'N':29940, 'ight':523, ' ow':8152, 'l':29880, ',':29892, ' I':306, ' love':5360, ' you':366, ',':29892, ' night':4646 ]
[1711052282] n_remain: -11
[1711052282] eval: [ ' night':4646 ]
[1711052344] 
[1711052345] llama_print_timings:        load time =   31392.10 ms
[1711052345] llama_print_timings:      sample time =     749.74 ms /    10 runs   (   74.97 ms per token,    13.34 tokens per second)
[1711052345] llama_print_timings: prompt eval time =   60122.08 ms /    82 tokens (  733.20 ms per token,     1.36 tokens per second)
[1711052345] llama_print_timings:        eval time =  483153.21 ms /     9 runs   (53683.69 ms per token,     0.02 tokens per second)
[1711052345] llama_print_timings:       total time =  614938.18 ms
